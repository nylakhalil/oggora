services:
  nvidia:
    profiles: ["gpu"]
    container_name: oggora-gpu
    image: nvidia/cuda:12.8.1-cudnn-runtime-ubuntu24.04
    command: nvidia-smi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  tensorflow:
    profiles: ["tf"]
    build:
      context: ./tensorflow
    container_name: oggora-tf
    image: oggora-tf:latest
    command: sh -c "/tmp/init.sh"
    ports:
      - 8888:8888
    volumes:
      - ./tensorflow/share:/tf
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          cpus: '2'
          memory: 16G
        limits:
          cpus: '4'
          memory: 24G
  pytorch:
    profiles: ["pt"]
    build:
      context: ./pytorch
      args:
        CUDA_VERSION: '12.8.1'
        IMAGE_OS: 'ubuntu'
        IMAGE_TYPE: '-cudnn-runtime-'
        IMAGE_VERSION: '24.04'
    container_name: oggora-pt
    image: oggora-pt:latest
    command: sh -c "/tmp/init.sh"
    ports:
      - 8888:8888
    volumes:
      - ./pytorch/share:/pt
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          cpus: '2'
          memory: 16G
        limits:
          cpus: '4'
          memory: 24G
    networks:
      default:
networks:
  default: